model_dir: enfr/exp_jun28/model 

data:
  train_features_file: enfr/exp_jun28/data/train.en
  train_labels_file: enfr/exp_jun28/data/train.fr
  eval_features_file: enfr/exp_jun28/data/dev.en
  eval_labels_file: enfr/exp_jun28/data/dev.fr
  source_words_vocabulary: enfr/exp_jun28/data/vocab.en
  target_words_vocabulary: enfr/exp_jun28/data/vocab.fr
  source_tokenizer_config: enfr/exp_jun28/config/tok_en_bpe.yml
  target_tokenizer_config: enfr/exp_jun28/config/tok_fr_bpe.yml

params:
  optimizer: GradientDescentOptimizer
  learning_rate: 1.0
  param_init: 0.1
  clip_gradients: 5.0
  decay_type: exponential_decay
  decay_rate: 0.7
  decay_steps: 100000
  start_decay_steps: 500000
  beam_width: 5
  maximum_iterations: 250

train:
  batch_size: 64
  bucket_width: 1
  save_checkpoints_steps: 5000
  save_summary_steps: 50
  train_steps: 1000000
  maximum_features_length: 50
  maximum_labels_length: 50

  # Consider setting this to -1 to match the number of training examples.
  sample_buffer_size: 1000000

eval:
  eval_delay: 18000  # Every 5 hours.

infer:
  batch_size: 30
